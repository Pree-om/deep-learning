{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1n9LgBVvSW8fD4kndcpmwt892DdPQftJS",
      "authorship_tag": "ABX9TyMy3ilNfJiEoxJzTZZfpB05",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pree-om/deep-learning/blob/main/ANN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "V296SolNNoKB"
      },
      "outputs": [],
      "source": [
        "#import required libraries\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "def sigmoid_derivative(x):\n",
        "  return x*(1-x)"
      ],
      "metadata": {
        "id": "CcMT-awtOgaG"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Constructor to initialize the weights and biases\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
        "        self.bias_hidden = np.random.rand(hidden_size)\n",
        "        self.bias_output = np.random.rand(output_size)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Calculate hidden layer output\n",
        "        self.hidden_layer_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Calculate output layer output\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output = sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output  # Add return statement to get output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Calculate the error at the output layer\n",
        "        output_error = self.output - y\n",
        "        output_delta = 2 * output_error * sigmoid_derivative(self.output)\n",
        "\n",
        "        # Calculate the error in the hidden layer\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output -= self.hidden_layer_output.T.dot(output_delta) * learning_rate\n",
        "        self.bias_output -= np.sum(output_delta, axis=0) * learning_rate  # Fixed the bias update\n",
        "\n",
        "        self.weights_input_hidden -= X.T.dot(hidden_delta) * learning_rate\n",
        "        self.bias_hidden -= np.sum(hidden_delta, axis=0) * learning_rate  # Add bias update for hidden layer\n",
        "\n",
        "    def train(self, X, y, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            self.feedforward(X)\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(self.output - y))  # Compute Mean Squared Error loss\n",
        "                print(f\"Epoch: {epoch}, Loss: {loss}\")"
      ],
      "metadata": {
        "id": "LAsDhf-zjuag"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution starts here\n",
        "if __name__ == '__main__':\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # 4 rows, each with 2 values\n",
        "    Y = np.array([[0], [1], [1], [0]])  # 4 output values for XOR problem\n",
        "\n",
        "    nn = NN(2, 2, 1)  # Initialize neural network with 2 input neurons, 2 hidden neurons, 1 output neuron\n",
        "    nn.train(X, Y, 0.1, 10000)  # Train the network\n",
        "\n",
        "    print(\"Final output:\")\n",
        "    print(nn.feedforward(X))  # Print the final output after training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT2_QsFzS2DD",
        "outputId": "7324876b-dfc5-4fd2-8e84-c75e86f3f9c4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.2820442033002448\n",
            "Epoch: 1000, Loss: 0.25001469838798995\n",
            "Epoch: 2000, Loss: 0.25000248863318797\n",
            "Epoch: 3000, Loss: 0.24999656781100027\n",
            "Epoch: 4000, Loss: 0.24997841083124556\n",
            "Epoch: 5000, Loss: 0.2498838038478915\n",
            "Epoch: 6000, Loss: 0.24905959671561662\n",
            "Epoch: 7000, Loss: 0.20695402588412068\n",
            "Epoch: 8000, Loss: 0.13951991986564416\n",
            "Epoch: 9000, Loss: 0.1313521768286075\n",
            "Final output:\n",
            "[[0.06043213]\n",
            " [0.49405818]\n",
            " [0.94688508]\n",
            " [0.50326468]]\n"
          ]
        }
      ]
    }
  ]
}